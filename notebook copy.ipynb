{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[8, 2, 8, ..., 3, 6, 7],\n",
       "        [4, 9, 8, ..., 8, 9, 5],\n",
       "        [6, 3, 8, ..., 9, 4, 4],\n",
       "        ...,\n",
       "        [5, 5, 7, ..., 9, 1, 2],\n",
       "        [1, 5, 9, ..., 9, 8, 1],\n",
       "        [2, 7, 7, ..., 8, 6, 9]]),\n",
       " array([[0, 0, 1, ..., 0, 1, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 1, ..., 0, 0, 0],\n",
       "        [1, 0, 0, ..., 1, 0, 0],\n",
       "        [0, 0, 1, ..., 0, 0, 0]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_cells = 1000\n",
    "n_genes = 10\n",
    "p_zero = 0.8 # probability that a gene expression value is zero\n",
    "\n",
    "\n",
    "rng = np.random.default_rng(seed=32)  # Create a generator with a seed\n",
    "x_pre = rng.integers(1, 10, size=(n_cells,n_genes))  # Generate 10 random integers between 0 and 10\n",
    "mask = rng.binomial(n=1, p=1-p_zero, size=(n_cells,n_genes))\n",
    "\n",
    "x_pre, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 8, ..., 0, 6, 0],\n",
       "       [0, 9, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 7, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 9, 0, 0],\n",
       "       [0, 0, 7, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x_pre * mask\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 3, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 7, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 9, 0, 0],\n",
       "       [0, 0, 7, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:n_cells//2,:] += 10\n",
    "x *= mask\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZINBLoss(nn.Module):\n",
    "    def __init__(self, ridge_lambda=0.0):\n",
    "        super(ZINBLoss, self).__init__()\n",
    "        self.eps = 1e-10\n",
    "        self.ridge_lambda = ridge_lambda\n",
    "\n",
    "    def forward(self, x, mean, dispersion, pi, scale_factor=1.0):\n",
    "        x = x.float()\n",
    "        mean = mean * scale_factor\n",
    "\n",
    "        nb_case = (\n",
    "            torch.lgamma(dispersion + self.eps)\n",
    "            + torch.lgamma(x + 1.0)\n",
    "            - torch.lgamma(x + dispersion + self.eps)\n",
    "            - dispersion * torch.log(dispersion + self.eps)\n",
    "            - x * torch.log(mean + self.eps)\n",
    "            + (dispersion + x) * torch.log(dispersion + mean + self.eps)\n",
    "        )\n",
    "\n",
    "        zero_case = -torch.log(pi + ((1.0 - pi) * torch.exp(-nb_case)) + self.eps)\n",
    "\n",
    "        result = torch.where(torch.lt(x, 1e-8), zero_case, -torch.log(1.0 - pi + self.eps) + nb_case)\n",
    "        ridge = self.ridge_lambda * (pi ** 2).sum()\n",
    "\n",
    "        return result.mean() + ridge\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc_mean = nn.Linear(hidden_dim, output_dim)      # Mean of NB\n",
    "        self.fc_disp = nn.Linear(hidden_dim, output_dim)      # Dispersion\n",
    "        self.fc_pi = nn.Linear(hidden_dim, output_dim)        # Zero-inflation\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = F.relu(self.fc1(z))\n",
    "        mean = torch.exp(self.fc_mean(h))                     # Mean > 0\n",
    "        dispersion = torch.exp(self.fc_disp(h))               # Dispersion > 0\n",
    "        pi = torch.sigmoid(self.fc_pi(h))                     # pi in [0, 1]\n",
    "        return mean, dispersion, pi\n",
    "\n",
    "\n",
    "class ZINBVAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(ZINBVAE, self).__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim, input_dim)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        mean, disp, pi = self.decoder(z)\n",
    "        return mean, disp, pi, mu, logvar, z\n",
    "\n",
    "    def loss_function(self, x, mean, disp, pi, mu, logvar):\n",
    "        zinb_loss = ZINBLoss()(x, mean, disp, pi)\n",
    "        kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)\n",
    "        return zinb_loss + kl_div\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # For deterministic behavior\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "    #torch.backends.cudnn.benchmark = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.0776\n",
      "Epoch 1, Loss: 2.0428\n",
      "Epoch 2, Loss: 2.0035\n",
      "Epoch 3, Loss: 1.9675\n",
      "Epoch 4, Loss: 1.9350\n",
      "Epoch 5, Loss: 1.9070\n",
      "Epoch 6, Loss: 1.8676\n",
      "Epoch 7, Loss: 1.8429\n",
      "Epoch 8, Loss: 1.8177\n",
      "Epoch 9, Loss: 1.7863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7653/545207868.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# Set the seed\n",
    "set_seed(11)\n",
    "vae = ZINBVAE(input_dim=n_genes, hidden_dim=20, latent_dim=5)\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "vae.train()\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    mean, disp, pi, mu, logvar, z = vae(x)\n",
    "    loss = vae.loss_function(x, mean, disp, pi, mu, logvar)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 5])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
